from __future__ import annotations
import logging
import os
import json

from pathlib import Path
from typing import Any, Dict, List, Optional, Iterable, Union

import yaml

from schema import SCHEMA_VERSION
from schema.eval_types import (
	EvaluationResult,
	ModelInfo,
	Configuration,
	GenerationArgs,
	InferenceSettings,
	Instance,
	Output,
	Evaluation,
	EvaluationMethod,
	PromptConfig,
	PromptClass,
	TaskType,
	SampleIdentifier,
	Quantization,
	Model,
	InstructionPhrasing,
	QuantizationType,
)

from eval_converters.common.adapter import BaseEvaluationAdapter, AdapterMetadata, SupportedLibrary
from eval_converters.common.utils import detect_family, detect_hf_split, infer_quantization, extract_context_window_from_config
from .utils import detect_prompt_class, MAIN_METRIC_BY_TASK, MULTIPLE_CHOICE_TASKS

# Core Adapter

class LMEvalAdapter(BaseEvaluationAdapter):

	# dumped config file: config.yaml 
	# samples file: samples_TIMESTAMP.jsonl generated by lm-eval during execution
	# results file: results_TIMESTAMP.json generated by lm-eval during execution

	CONFIG_FILE = "config.yaml"
	RESULTS_FILE = "results.json"
	SAMPLES_FILE = "samples.jsonl"

	@property
	def metadata(self) -> AdapterMetadata:
		return AdapterMetadata(
			name="LMEvalAdapter",
			version="0.0.1",
			supported_library_versions=["0.4.0", "0.5.0", "0.5.1"],
			description="Adapter for transforming LM-Eval evaluation outputs to unified schema format"
		)

	@property
	def supported_library(self) -> SupportedLibrary:
		return SupportedLibrary.LM_EVAL
	
	def _load_file(self, file_path: Path) -> Any:
		if file_path.suffix == ".jsonl":
			return [json.loads(line) for line in file_path.read_text().splitlines()]

	def transform_from_directory(self, dir_path: Union[str, Path]) -> List[EvaluationResult]:
		dir_path = Path(dir_path)
		if not dir_path.is_dir():
			raise FileNotFoundError(f"Directory {dir_path} does not exist")

		cfg_path = os.path.join(dir_path, self.CONFIG_FILE)
		cfg: Dict[str, Any] = {}
		if os.path.exists(cfg_path):
			with open(cfg_path, "r", encoding="utf-8") as f:
				cfg = yaml.safe_load(f)

		else:
			logging.warning(f"config.yaml not found - falling back to default config")

		# Extract model name from config or fallback to directory name
		if isinstance(cfg.get("model_args"), dict):
			model_name = cfg["model_args"].get("pretrained", cfg.get("model", "unknown-model"))

		elif isinstance(cfg.get("model_args"), str) and "pretrained=" in cfg.get("model_args", ""):
			# Extract from string format: "pretrained=gpt2,dtype=float32"
			for part in cfg["model_args"].split(","):
				if part.strip().startswith("pretrained="):
					model_name = part.split("=", 1)[1].strip()
					break
			else:
				model_name = cfg.get("model", "unknown-model")
		else:
			# Fallback to directory name if no model info in config
			model_name = dir_path.name if dir_path.name != "." else "unknown-model"

		precision, quant_method, quant_type = infer_quantization(model_name)

		generation_args = GenerationArgs(
			temperature = cfg.get("temperature", 0.0),
			top_p = cfg.get("top_p", 1.0),
			top_k = cfg.get("top_k", 20),
			max_tokens = cfg.get("max_tokens"),
		)

		inference_settings = InferenceSettings(
			quantization = Quantization(bit_precision=precision, method=quant_method, type=quant_type),
			generation_args = generation_args,
		)

		context_window = extract_context_window_from_config(model_name)

		model_block = Model(
			model_info = ModelInfo(
				name=model_name,
				provider = (model_name.split("/", 1)[0] if "/" in model_name else "unknown"),
				family = detect_family(model_name),
			),
			configuration = Configuration(
				context_window = context_window,
			),
			inference_settings = inference_settings,
		)

		# Load task-level metrics
		task_scores: Dict[str, Dict[str, float]] = {}
		results_path = self._find_first_file(dir_path, [self.RESULTS_FILE])

		if results_path:
			with open(results_path, "r", encoding="utf-8") as f:
				results = json.load(f)

		# Enumerate per-instance samples
		pred_path = self._find_first_file(dir_path, [self.SAMPLES_FILE])
		if pred_path is None:
			raise FileNotFoundError("No samples file found")

		evaluations: List[EvaluationResult] = []
		with open(pred_path, "r", encoding="utf-8") as f:
			for line_idx, line in enumerate(f):
				if not line.strip():
					continue

				record = json.loads(line)

				# Extract task name from record, config, or filename
				task_name = record.get("task")
				if not task_name:
					# Try from config
					tasks = cfg.get("tasks", [])
					if tasks:
						task_name = tasks[0]
					else:
						# Try to extract from filename (e.g., "samples_hellaswag_2025...")
						filename = pred_path.name
						if "_" in filename:
							parts = filename.split("_")
							for part in parts:
								if part in MAIN_METRIC_BY_TASK or part.lower() in MULTIPLE_CHOICE_TASKS:
									task_name = part
									break
				
				if not task_name:
					task_name = "unknown_task"
				prompt_class = detect_prompt_class(task_name)

				# Provide a default instruction phrasing so PromptConfig validates
				if prompt_class == PromptClass.MultipleChoice:
					instruction_text = "Choose the correct answer from the options."
				elif prompt_class == PromptClass.OpenEnded:
					instruction_text = "Provide a helpful, concise answer."
				else:
					instruction_text = "Complete the prompt appropriately."
				default_phrasing = InstructionPhrasing(name="default", text=instruction_text)

				raw_inp = record.get("input") or record.get("question") or record.get("ctx") or ""
				ground_truth = record.get("label") or record.get("answers", [None])[0] or record.get("target") or ""
				prediction = record.get("prediction") or record.get("pred") or record.get("decoded") or record.get("response") or ""

				# compute 0/1 correctness if not provided
				if "correct" in record:
					score = 1.0 if record["correct"] else 0.0
				else:
					# some tasks provide numeric labels - normalize to str equality
					score = 1.0 if str(prediction).strip() == str(ground_truth).strip() else 0.0

				# Allow task-level main metric override
				metric_name = MAIN_METRIC_BY_TASK.get(task_name, "accuracy")
				if metric_name in record:
					# use per-instance metric score if available (e.g., acc_norm for hellaswag)
					score = record[metric_name]

				evaluation = Evaluation(
					evaluation_method = EvaluationMethod(
						method_name = "lm-eval-harness",
						description = "0-1 correctness computed from per-instance prediction",
					),
					ground_truth = str(ground_truth),
					score = score,
					classification_fields = self._build_classification_fields(record, ground_truth) if prompt_class == PromptClass.MultipleChoice else None
				)

				instance = Instance(
					task_type = TaskType.classification if prompt_class == PromptClass.MultipleChoice else TaskType.generation,
					raw_input = str(raw_inp),
					language = "en",					# harness tasks are predominantly English - override if known 
					sample_identifier = SampleIdentifier(
						dataset_name = task_name,
						hf_repo = "", # not available in harness
						hf_split = detect_hf_split(record.get("split", "test")),
						hf_index = int(record.get("idx", line_idx)),
					),
					classification_fields = self._build_classification_fields(record, ground_truth) if prompt_class == PromptClass.MultipleChoice else None
				)

				evaluations.append(EvaluationResult(
					schema_version = SCHEMA_VERSION,
					evaluation_id = f"{task_name}:{record.get("idx", line_idx)}",
					model = model_block,
					prompt_config = PromptConfig(prompt_class=prompt_class, instruction_phrasing=default_phrasing),
					instance = instance,
					output = Output(response = str(prediction)),
					evaluation = evaluation,
				))
			
		return evaluations

	@staticmethod
	def _build_classification_fields(rec: Dict[str, Any], ground_truth: str) -> Dict[str, Any]:
		choices = rec.get("choices") or rec.get("options") or rec.get("mc_options")
		if choices and isinstance(choices, (list, tuple)):
			formatted = [
				{"id": str(i), "text": str(c)} for i,c in enumerate(choices)
			]

			return {
				"full_input": rec.get("input") or rec.get("question") or rec.get("ctx") or "",
				"question": rec.get("question") or rec.get("input", ""),
				"choices": formatted,
				"ground_truth": {"id": str(ground_truth), "text": choices[int(ground_truth)]} if isinstance(ground_truth, int) else {"text": ground_truth},
			}

		return {}

	@staticmethod
	def _find_first_file(root: Path, names: Iterable[str]) -> Optional[Path]:
		import glob
		
		# First try exact matches
		for name in names:
			p = root / name
			if p.exists():
				return p
		
		# Then try glob patterns for timestamped files based on requested file type
		for name in names:
			if name.endswith('.jsonl'):
				# Looking for samples file
				matches = list(root.glob("samples_*.jsonl"))
				if matches:
					return matches[0]
			elif name.endswith('.json'):
				# Looking for results file
				matches = list(root.glob("results_*.json"))
				if matches:
					return matches[0]

		# recursively search one level down (each task often has its own folder)
		for sub_dir in root.iterdir():
			if sub_dir.is_dir():
				# Try exact names first
				for name in names:
					p = sub_dir / name
					if p.exists():
						return p
				
				# Try patterns in subdirectories based on requested file type
				for name in names:
					if name.endswith('.jsonl'):
						matches = list(sub_dir.glob("samples_*.jsonl"))
						if matches:
							return matches[0]
					elif name.endswith('.json'):
						matches = list(sub_dir.glob("results_*.json"))
						if matches:
							return matches[0]

		return None

	def _transform_single(self, raw_data: Union[str, Dict[str, Any]]) -> List[EvaluationResult]:
		# lm_eval already works with a single config yaml file,
		# so, we don't need to support single-dict transform, if given path -> transform_from_directory
		
		if isinstance(raw_data, dict):
			raise ValueError("Single-dict transform is unsupported")

		if isinstance(raw_data, str) and os.path.isfile(raw_data):
			tmp_dir = Path(raw_data).parent
			return self.transform_from_directory(tmp_dir)

		raise ValueError(f"Unsupported raw_data type for LMEvalAdapter: {type(raw_data)}")


	
