# generated by datamodel-codegen:
#   filename:  leaderboard.schema.json
#   timestamp: 2025-09-29T19:52:18+00:00

from __future__ import annotations

from enum import Enum
from typing import List, Optional

from pydantic import BaseModel, ConfigDict, Field, conint


class GenerationArgs(BaseModel):
    model_config = ConfigDict(
        extra='allow',
    )
    temperature: Optional[float] = Field(None, description='Sampling temperature')
    top_p: Optional[float] = Field(None, description='Nucleus sampling parameter')
    top_k: Optional[float] = Field(None, description='Top-k sampling parameter')
    max_tokens: Optional[conint(ge=1)] = Field(
        None, description='Maximum number of tokens to generate'
    )


class InferenceSettings(BaseModel):
    quantization_method: Optional[str] = Field(
        None, description='Quantization method used for the model (e.g GPTQ)'
    )
    generation_args: Optional[GenerationArgs] = None


class ModelInfo(BaseModel):
    name: str = Field(
        ..., description="Model name and version (e.g., 'Llama-2-13b-chat-hf')"
    )
    source_url: str = Field(
        ..., description='URL for the source of the evaluation data'
    )
    provider_name: Optional[str] = Field(
        None,
        description='Name of the provider for the version of the model used during evaluation.',
    )
    developer: Optional[str] = Field(
        None, description="Name of organization that provides the model (e.g. 'OpenAI')"
    )
    inference_settings: Optional[InferenceSettings] = Field(
        None,
        description='Runtime settings and parameters for model inference - controls how the model generates outputs and performs during execution',
    )


class ScoreType(Enum):
    binary = 'binary'
    continuous = 'continuous'
    levels = 'levels'


class MetricConfig(BaseModel):
    evaluation_description: Optional[str] = Field(
        None, description='Description of the evaluation'
    )
    lower_is_better: bool = Field(..., description='Whether a lower score is better')
    score_type: Optional[ScoreType] = Field(None, description='Type of score')
    score_level_names: Optional[List[str]] = Field(
        None, description='Names of the score levels'
    )
    min_score: Optional[float] = Field(None, description='Minimum possible score')
    max_score: Optional[float] = Field(None, description='Maximum possible score')


class ScoreDetails(BaseModel):
    score: float = Field(..., description='The score for the evaluation')
    details: Optional[str] = Field(
        None, description='Any additional details about the score'
    )


class SampleLevelDatum(BaseModel):
    sample_id: str = Field(..., description='Unique identifier for the sample')
    score: float = Field(..., description='Score for the sample')


class EvaluationResult(BaseModel):
    evaluation_name: str = Field(..., description='Name of the evaluation')
    metric_config: MetricConfig = Field(..., description='Details about the metric')
    score_details: ScoreDetails = Field(
        ..., description='The score for the evaluation and related details'
    )
    sample_level_data: Optional[List[SampleLevelDatum]] = Field(
        None, description='Sample level results for items used in evaluation'
    )
    generation_config: Optional[str] = Field(
        None, description='Details about how the scores were generated'
    )


class LeaderboardEvaluationResult(BaseModel):
    schema_version: str = Field(
        ..., description='Version of the schema used for this evaluation data'
    )
    evaluation_id: str = Field(
        ..., description='Unique identifier for this specific evaluation run'
    )
    model_info: ModelInfo = Field(
        ...,
        description='Complete model specification including basic information, technical configuration and inference settings',
    )
    evaluation_results: List[EvaluationResult] = Field(
        ..., description='Array of evaluation results'
    )
