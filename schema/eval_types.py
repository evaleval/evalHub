# generated by datamodel-codegen:
#   filename:  eval.schema.json
#   timestamp: 2025-07-06T13:59:25+00:00

from __future__ import annotations

from enum import Enum
from typing import Annotated, Any, Dict, List, Optional, Union

from pydantic import BaseModel, ConfigDict, Field


class Family(Enum):
    Llama = 'Llama'
    Mistral = 'Mistral'
    OLMo = 'OLMo'
    gemma = 'gemma'
    gpt = 'gpt'
    palm = 'palm'
    claude = 'claude'
    falcon = 'falcon'
    Qwen = 'Qwen'
    NoneType_None = None


class ModelInfo(BaseModel):
    name: str = Field(
        ..., description="Model name and version (e.g., 'Llama-2-13b-chat-hf')"
    )
    provider: str = Field(
        ..., description="Name of the provider that shared the model used for evaluation'"
    )
    family: Optional[Family] = Field(None, description='Model family')


class Architecture(Enum):
    transformer = 'transformer'
    moe = 'moe'
    ssm = 'ssm'
    NoneType_None = None


class Configuration(BaseModel):
    architecture: Optional[Architecture] = Field(
        None, description='Model architecture type'
    )
    parameters: Optional[Annotated[int, Field(ge=1)]] = Field(
        None, description='Number of parameters in billions'
    )
    context_window: Optional[Annotated[int, Field(ge=1)]] = Field(
        ..., description='Maximum context window size in tokens'
    )
    is_instruct: Optional[bool] = Field(
        None, description='Whether the model is instruction-tuned'
    )
    hf_path: Optional[str] = Field(None, description='HuggingFace model path')
    revision: Optional[str] = Field(None, description='Model revision/commit hash')


class BitPrecision(Enum):
    none = 'none'
    int8 = 'int8'
    int4 = 'int4'
    float16 = 'float16'
    float32 = 'float32'


class QuantizationType(Enum):
    none = 'None'
    dynamic = 'dynamic'
    static = 'static'

class QuantizationMethod(Enum):
    awq = 'AWQ'
    gptq = 'GPTQ'
    none = 'None'


class Quantization(BaseModel):
    bit_precision: Optional[BitPrecision] = Field(..., description='Quantization bit precision')
    method: Optional[QuantizationMethod] = Field(..., description='Quantization type (algorithm) like gptq, awq, bitsandbyted, so on...')
    type: Optional[QuantizationType] = Field(..., description='Quantization type (static or dynamic)')


class GenerationArgs(BaseModel):
    use_vllm: Optional[bool] = Field(
        None, description='Whether VLLM was used for inference'
    )
    temperature: Optional[float] = Field(None, description='Sampling temperature')
    top_p: Optional[float] = Field(None, description='Nucleus sampling parameter')
    top_k: Optional[float] = Field(None, description='Top-k sampling parameter')
    max_tokens: Optional[Annotated[int, Field(ge=1)]] = Field(
        None, description='Maximum number of tokens to generate'
    )
    stop_sequences: Optional[List[str]] = Field(
        [], description='Sequences that stop generation'
    )
    seed: Optional[float] = Field(
        5.0, description='Random seed'
    )
    frequency_penalty: Optional[float] = Field(
        0.0, description='Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim'
    )
    presence_penalty: Optional[float] = Field(
        0.0, description='Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics.'
    )
    logit_bias: Optional[Dict[int, float]] = Field(
        0.0, description='Map token Ids to an associated bias value'
    )
    logprobs: Optional[bool] = Field(
        False, description='Return log probabilities of the output tokens'
    )
    top_logprobs: Optional[int] = Field(
        1, description='Number of most likely tokens (0-20) to return at each token position'
    )


class InferenceSettings(BaseModel):
    quantization: Quantization
    generation_args: Optional[GenerationArgs] = None


class Model(BaseModel):
    model_info: ModelInfo = Field(
        ...,
        description='Basic identifying information about the model - represents the core identity and naming of the model without technical details',
    )
    configuration: Configuration = Field(
        ...,
        description="Technical specifications and implementation details of the model - defines how the model is structured and where it's hosted",
    )
    inference_settings: InferenceSettings = Field(
        ...,
        description='Runtime settings and parameters for model inference - controls how the model generates outputs and performs during execution',
    )


class PromptClass(Enum):
    MultipleChoice = 'MultipleChoice'
    OpenEnded = 'OpenEnded'
    Completion = 'Completion'


class ChoicesOrder(BaseModel):
    method: str = Field(..., description='The method to use for ordering choices')
    description: str = Field(
        ..., description='Detailed explanation of the ordering method'
    )


class Enumerator(Enum):
    capitals = 'capitals'
    lowercase = 'lowercase'
    numbers = 'numbers'
    roman = 'roman'
    keyboard = 'keyboard'
    greek = 'greek'


class InstructionPhrasing(BaseModel):
    name: str = Field(..., description='Name of the instruction template')
    text: str = Field(
        ...,
        description='Template text with placeholders for question and choices (or more)',
    )


class Separator(Enum):
    field_s = '\\s'
    field_ = '\n'
    field__ = ', '
    field___1 = '; '
    field___ = ' | '
    field_OR_ = ' OR '
    field_or_ = ' or '


class Dimensions(BaseModel):
    choices_order: ChoicesOrder
    demonstrations: Optional[List] = Field(
        [], description='Array of demonstration examples used in few-shot prompting'
    )
    enumerator: Enumerator = Field(
        ..., description='Style of enumeration for multiple choice options'
    )
    separator: Separator = Field(
        ..., description='Character(s) used to separate multiple choice options'
    )
    shots: Optional[Annotated[int, Field(ge=0, le=10)]] = Field(
        ..., description='Number of examples provided in the prompt'
    )


class PromptConfig(BaseModel):
    prompt_class: PromptClass = Field(
        ..., description='Type of task and its formatting requirements'
    )
    instruction_phrasing: InstructionPhrasing
    dimensions: Optional[Dimensions] = Field(
        None, description='Format-specific configuration dimensions'
    )


class TaskType(Enum):
    classification = 'classification'
    generation = 'generation'


class RawInputItem(BaseModel):
    role: str
    content: str


class HfSplit(Enum):
    train = 'train'
    test = 'test'
    validation = 'validation'


class SampleIdentifier(BaseModel):
    dataset_name: str = Field(..., description='Name of the source dataset')
    hf_repo: str = Field(..., description='HuggingFace repository identifier')
    hf_index: int = Field(..., description='Index in the HuggingFace dataset')
    hf_split: Optional[HfSplit] = Field(..., description='HuggingFace split identifier')

class PromptLogprob(BaseModel):
    token_id: float = Field(
        ..., description='Index position of the token in the sequence'
    )
    logprob: float = Field(..., description='Log probability of the token')
    rank: int = Field(..., description="Rank of the token in the model's vocabulary")
    decoded_token: str = Field(
        ..., description='The decoded string representation of the token'
    )


class Instance(BaseModel):
    model_config = ConfigDict(
        extra='allow',
    )
    task_type: TaskType = Field(..., description='Type of the task')
    raw_input: Union[str, List[RawInputItem]]
    language: str = Field(
        ...,
        description='Language of the input text',
        examples=[
            'en',
            'fr',
            'de',
            'es',
            'it',
            'pt',
            'nl',
            'pl',
            'ru',
            'tr',
            'ar',
            'ja',
            'ko',
            'zh',
            'he',
        ],
    )
    sample_identifier: SampleIdentifier = Field(
        ..., description='Metadata to help identify and match the sample to the dataset'
    )
    classification_fields: Optional[Dict[str, Any]] = Field(
        None, description='Fields specific to classification tasks'
    )
    prompt_logprobs: Optional[List[Union[Optional[str], List[PromptLogprob]]]] = Field(
        None,
        description="Array of token probability data, with first element being 'None' and subsequent elements being arrays of token information",
    )


class GeneratedTokensLogprob(BaseModel):
    token_id: float = Field(
        ..., description='Index position of the token in the sequence'
    )
    logprob: float = Field(..., description='Log probability of the token')
    rank: int = Field(..., description="Rank of the token in the model's vocabulary")
    decoded_token: str = Field(
        ..., description='The decoded string representation of the token'
    )


class Output(BaseModel):
    response: str = Field(..., description="The model's complete text response")
    cumulative_logprob: Optional[float] = Field(
        None,
        description='Cumulative log probability of the generated sequence from VLLM',
    )
    generated_tokens_logprobs: Optional[
        List[Union[Optional[str], List[GeneratedTokensLogprob]]]
    ] = None


class EvaluationMethod(BaseModel):
    method_name: str = Field(
        ...,
        description='Name of the evaluation method. Can be a predefined method or a user-defined method.',
    )
    description: str = Field(
        ...,
        description='Detailed explanation of how the evaluation method works. For user-defined methods, this is required.',
    )
    parameters: Optional[Dict[str, Any]] = Field(
        None,
        description='Optional parameters used by the evaluation method. Allows custom configuration.',
    )


class Evaluation(BaseModel):
    evaluation_method: EvaluationMethod = Field(
        ...,
        description='Method used to evaluate the answer, including predefined methods and user-defined methods.',
    )
    ground_truth: str = Field(..., description='The correct answer for evaluation')
    score: Annotated[float, Field(gt=0.0, le=1.0)] = Field(
        ...,
        description="Binary score indicating whether the model's answer was correct (1.0) or incorrect (0.0)",
    )


class EvaluationResult(BaseModel):
    schema_version: str = Field(
        ..., description='Version of the schema used for this evaluation data'
    )
    evaluation_id: str = Field(
        ...,
        description='Unique identifier for this specific evaluation run, including the model configuration, prompt, instance, output and evaluation results',
    )
    model: Model = Field(
        ...,
        description='Complete model specification including basic information, technical configuration and inference settings',
    )
    prompt_config: PromptConfig = Field(
        ..., description='Configuration of the prompt template and formatting'
    )
    instance: Instance = Field(..., description='Specific instance and its metadata')
    output: Output = Field(
        ..., description="Model's response and associated probability metrics"
    )
    evaluation: Evaluation = Field(
        ..., description='Evaluation metrics and ground truth'
    )
