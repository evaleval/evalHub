# EvalHub Infrastructure

This repository provides a unified and extensible framework for running and organizing evaluations across multiple LLM evaluation tools such as [`lm-evaluation-harness`](https://github.com/EleutherAI/lm-evaluation-harness), [`HELM`](https://github.com/stanford-crfm/helm), etc.

## Prerequisites

- Python 3.12 or higher
- [`uv`](https://docs.astral.sh/uv/)

## Installation

- Install the required dependencies:

```bash
uv sync
```

## Development

Schema is defined by file ```eval.schema.json``` in schema directory. Look on README in this file for more details.

## Run pipelines

# Inspect AI

```python
uv run python3 -m eval_converters.inspect.converter --log_path --output_dir
```
where
```log_path``` - evaluation log file generated by Inspect AI (documentation https://inspect.aisi.org.uk/eval-logs.html#sec-log-format)
```output_dir``` - directory to save unified schema, by default it is 'unified_schema/inspect_ai'

To test it run below command on the sample evaluation log:

```python
uv run python3 -m eval_converters.inspect.converter --log_path tests/data/inspectai/data.json 
```